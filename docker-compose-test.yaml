version: '2'
services:
    postgres:
        image: postgres:9.6
        environment:
            - POSTGRES_USER=airflow
            - POSTGRES_PASSWORD=airflow
            - POSTGRES_DB=airflow

    webserver:
        image: gfw/docker-airflow
        build: .
        restart: always
        depends_on:
            - postgres
        environment:
            - LOAD_EX=n
            - EXECUTOR=Local
            - AIRFLOW_HOME=/usr/local/airflow
            # Note that this means that this must be run from airflow directory.
            - HOST_DAGS=${PWD}/temp-dags
            - DOCKER_USE_LOCAL=1

            # TEST_PIPELINE is used bu initialize.sh to run the post_install.sh script
            # for the automatically mounted pipeline
            - TEST_PIPELINE=${TEST_PIPELINE}
        volumes:
            - /var/run/docker.sock:/var/run/docker.sock
            - ${PWD}/temp-dags:/usr/local/airflow/dags
            - gcp:/root/.config/
            - ./initialize:/usr/local/airflow/initialize

            # Mount the test pipeline into the dags folder.  initialize.sh will run the
            # post_install script on startup
            - ${TEST_PIPELINE_DAGS}:/usr/local/airflow/dags/${TEST_PIPELINE}
        ports:
            - "8080:8080"
        command: webserver

# Use an external named volume so that we can share gcp auth across containers
# Before first use, this volume must be manually created with
#   docker volume create --name=gcp
volumes:
  gcp:
    external: true    

